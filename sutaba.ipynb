{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sutaba.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpppk/colaboratory/blob/master/sutaba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W6D3jtL-PmK",
        "colab_type": "code",
        "outputId": "c2732bd6-39ac-4395-9a59-ed7640496929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi6KAh2tExrV",
        "colab_type": "code",
        "outputId": "0a3ee2e3-a357-47f5-ee46-0372c92aad0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# !apt-get update && apt-get install imagemagick\n",
        "!cat /proc/uptime | awk '{print $1 /60 /60 /24 \"days (\" $1 \"sec)\"}'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/My Drive/ColabNotebooks/sutaba/models’: No such file or directory\n",
            "0.00185764days (160.50sec)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4f1ASW6_sqn",
        "colab_type": "code",
        "outputId": "b8abfa56-93d0-4223-c32f-0e8f90272b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Activation, Dropout, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# 分類するクラス\n",
        "classes = [\"sutaba\",\"ramen\", \"other\"]\n",
        "nb_classes = len(classes)\n",
        "#画像の大きさを設定\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# トレーニング用とバリデーション用の画像格納先（パスは自分で設定してください）\n",
        "base_path = \"/content/gdrive/My Drive/ColabNotebooks/sutaba\"\n",
        "train_data_dir = base_path + '/train'\n",
        "test_data_dir = base_path + '/test'\n",
        "aug_train_data_dir = base_path + '/aug_train'\n",
        "model_dir = base_path + '/models'\n",
        "# os.mkdir(aug_train_data_dir)\n",
        "\n",
        "#トレーニングデータ用の画像数\n",
        "def find_all_files(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            yield os.path.join(root, file)\n",
        "\n",
        "def count_file_num(p: str) -> int:\n",
        "  cnt = 0\n",
        "  for f in find_all_files(p):\n",
        "    cnt += 1\n",
        "  return cnt\n",
        "\n",
        "#バッチサイズ\n",
        "batch_size = 300\n",
        "#エポック数\n",
        "nb_epoch = 100\n",
        "validation_rate = 0.1\n",
        "\n",
        "sutaba_train_num = count_file_num(os.path.join(train_data_dir, classes[0]))\n",
        "ramen_train_num = count_file_num(os.path.join(train_data_dir, classes[1]))\n",
        "other_train_num = count_file_num(os.path.join(train_data_dir, classes[2]))\n",
        "class_weight = {0: other_train_num/sutaba_train_num, 1: other_train_num/ramen_train_num, 2: 1}\n",
        "nb_train_samples = sutaba_train_num + ramen_train_num + other_train_num\n",
        "\n",
        "sutaba_test_num = count_file_num(os.path.join(test_data_dir, classes[0]))\n",
        "ramen_test_num = count_file_num(os.path.join(test_data_dir, classes[1]))\n",
        "other_test_num = count_file_num(os.path.join(test_data_dir, classes[2]))\n",
        "nb_test_samples = sutaba_test_num + ramen_test_num + other_test_num\n",
        "\n",
        "print(sutaba_train_num, ramen_train_num, other_train_num, class_weight)\n",
        "print(sutaba_test_num, ramen_test_num, other_test_num)\n",
        "\n",
        "def get_latest_modified_file_path(dirname):\n",
        "  target = os.path.join(dirname, '*')\n",
        "  files = [(f, os.path.getmtime(f)) for f in glob(target)]\n",
        "  latest_modified_file_path = sorted(files, key=lambda files: files[1])[-1]\n",
        "  return latest_modified_file_path[0]\n",
        "\n",
        "def create_vgg16_from_weights(weights_path: str):\n",
        "  vgg16_model = create_vgg16()\n",
        "  vgg16_model.load_weights(get_latest_modified_file_path(weights_path))\n",
        "  return vgg16_model\n",
        "\n",
        "\n",
        "def create_vgg16():\n",
        "  # VGG16のロード。FC層は不要なので include_top=False\n",
        "  input_tensor = Input(shape=(img_width, img_height, 3))\n",
        "  vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
        "\n",
        "  # VGG16の図の緑色の部分（FC層）の作成\n",
        "  top_model = Sequential()\n",
        "  top_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\n",
        "  top_model.add(Dense(256, activation='relu'))\n",
        "  top_model.add(Dropout(0.5))\n",
        "  top_model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "  # VGG16とFC層を結合してモデルを作成（完成図が上の図）\n",
        "  # vgg_model = Model()\n",
        "  vgg_model = Model(input=vgg16.input, output=top_model(vgg16.output))\n",
        "\n",
        "  # VGG16の図の青色の部分は重みを固定（frozen）\n",
        "  for layer in vgg_model.layers[:15]:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # 多クラス分類を指定\n",
        "  vgg_model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=optimizers.SGD(lr=1e-3, momentum=0.9),\n",
        "            metrics=['accuracy'])\n",
        "  return vgg_model\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "  rescale=1.0 / 255,\n",
        "  #すでに画像の水増し済みの方は、下記２行は必要ありません。\n",
        "  zoom_range=0.2,\n",
        "  horizontal_flip=True,\n",
        "  validation_split=validation_rate\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "  train_data_dir,\n",
        "  target_size=(img_width, img_height),\n",
        "  color_mode='rgb',\n",
        "  classes=classes,\n",
        "  class_mode='categorical',\n",
        "  batch_size=batch_size,\n",
        "  # save_to_dir=aug_train_data_dir,\n",
        "  shuffle=True,\n",
        "  subset='training'\n",
        ")\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "  train_data_dir,\n",
        "  target_size=(img_width, img_height),\n",
        "  color_mode='rgb',\n",
        "  classes=classes,\n",
        "  class_mode='categorical',\n",
        "  batch_size=batch_size,\n",
        "  # save_to_dir=aug_train_data_dir,\n",
        "  shuffle=True,\n",
        "  subset='validation'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "900 550 2387 {0: 2.652222222222222, 1: 4.34, 2: 1}\n",
            "73 67 70\n",
            "Found 3454 images belonging to 3 classes.\n",
            "Found 383 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "490F074Jtv0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mogrify \\\n",
        "  -path /content/gdrive/'My Drive'/ColabNotebooks/sutaba/train/sutaba \\\n",
        "  -define jpeg:size=224x224 \\\n",
        "  -thumbnail 224x224^ \\\n",
        "  -gravity center \\\n",
        "  -extent 224x224 \\\n",
        "  /content/gdrive/'My Drive'/ColabNotebooks/sutaba/train/sutaba-original/*.jpg\n",
        "# convert -define jpeg:size=200x200 original.jpeg  -thumbnail 100x100^ -gravity center -extent 100x100  thumbnail.jpeg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7ecnLdrTt3p",
        "colab_type": "code",
        "outputId": "7a823626-0379-433c-faaf-d93abcda0460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# get_latest_modified_file_path(model_dir)\n",
        "import datetime\n",
        "n = datetime.datetime.now()\n",
        "nstr = f'{n.year}-{n.month:02}-{n.day:02}_{n.hour:02}-{n.minute:02}-{n.second:02}'\n",
        "print(nstr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-08-07_13-52-36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn_8XIv2ARY8",
        "colab_type": "code",
        "outputId": "782c73b0-9149-4555-c745-882d6f4890a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "vgg_model = create_vgg16_from_weights(model_dir)\n",
        "vgg_model.compile(loss='categorical_crossentropy',\n",
        "          optimizer=optimizers.SGD(lr=1e-3, momentum=0.9),\n",
        "          metrics=['accuracy'])\n",
        "n = datetime.datetime.now()\n",
        "nstr = f'{n.year}-{n.month:02}-{n.day:02}_{n.hour:02}-{n.minute:02}-{n.second:02}'\n",
        "fpath = base_path + f'/models/{nstr}' + 'weights.{epoch:02d}-{loss:.2f}-{acc:.2f}-{val_loss:.2f}-{val_acc:.2f}.hdf5'\n",
        "cp_cb = keras.callbacks.ModelCheckpoint(filepath=fpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "# Fine-tuning\n",
        "history = vgg_model.fit_generator(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    # samples_per_epoch=nb_train_samples,\n",
        "    samples_per_epoch=len(train_generator.classes),\n",
        "    nb_epoch=nb_epoch,\n",
        "    callbacks=[cp_cb],\n",
        "    nb_val_samples=len(validation_generator.classes),\n",
        "    class_weight=class_weight\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., callbacks=[<keras.ca..., class_weight={0: 2.6522..., steps_per_epoch=11, epochs=100, validation_steps=383)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 2506s 228s/step - loss: 0.3801 - acc: 0.8936 - val_loss: 0.3957 - val_acc: 0.8475\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.39572, saving model to /content/gdrive/My Drive/ColabNotebooks/sutaba/models/2019-08-07_13-54-18weights.01-0.38-0.90-0.40-0.85.hdf5\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 1234s 112s/step - loss: 0.3403 - acc: 0.9182 - val_loss: 0.4103 - val_acc: 0.8514\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.39572\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 1228s 112s/step - loss: 0.2726 - acc: 0.9317 - val_loss: 0.3929 - val_acc: 0.8573\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.39572 to 0.39292, saving model to /content/gdrive/My Drive/ColabNotebooks/sutaba/models/2019-08-07_13-54-18weights.03-0.27-0.93-0.39-0.86.hdf5\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 1230s 112s/step - loss: 0.2763 - acc: 0.9323 - val_loss: 0.3947 - val_acc: 0.8605\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.39292\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 1228s 112s/step - loss: 0.3252 - acc: 0.9117 - val_loss: 0.3771 - val_acc: 0.8611\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.39292 to 0.37707, saving model to /content/gdrive/My Drive/ColabNotebooks/sutaba/models/2019-08-07_13-54-18weights.05-0.32-0.92-0.38-0.86.hdf5\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 1233s 112s/step - loss: 0.2894 - acc: 0.9278 - val_loss: 0.4006 - val_acc: 0.8567\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.37707\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 1229s 112s/step - loss: 0.2904 - acc: 0.9294 - val_loss: 0.3758 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.37707 to 0.37584, saving model to /content/gdrive/My Drive/ColabNotebooks/sutaba/models/2019-08-07_13-54-18weights.07-0.29-0.93-0.38-0.86.hdf5\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 1231s 112s/step - loss: 0.2981 - acc: 0.9198 - val_loss: 0.4020 - val_acc: 0.8524\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.37584\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 1223s 111s/step - loss: 0.2969 - acc: 0.9301 - val_loss: 0.5274 - val_acc: 0.8152\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.37584\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 1227s 112s/step - loss: 0.2992 - acc: 0.9284 - val_loss: 0.4402 - val_acc: 0.8474\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.37584\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 1221s 111s/step - loss: 0.2535 - acc: 0.9347 - val_loss: 0.4019 - val_acc: 0.8587\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.37584\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 1235s 112s/step - loss: 0.2133 - acc: 0.9478 - val_loss: 0.4013 - val_acc: 0.8595\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.37584\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 1232s 112s/step - loss: 0.2111 - acc: 0.9507 - val_loss: 0.4650 - val_acc: 0.8446\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.37584\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 1222s 111s/step - loss: 0.2180 - acc: 0.9427 - val_loss: 0.5060 - val_acc: 0.8308\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.37584\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 1206s 110s/step - loss: 0.2329 - acc: 0.9372 - val_loss: 0.4430 - val_acc: 0.8505\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.37584\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 1213s 110s/step - loss: 0.1834 - acc: 0.9594 - val_loss: 0.4364 - val_acc: 0.8596\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.37584\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 1215s 110s/step - loss: 0.1821 - acc: 0.9563 - val_loss: 0.4139 - val_acc: 0.8633\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.37584\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 1218s 111s/step - loss: 0.1891 - acc: 0.9562 - val_loss: 0.4308 - val_acc: 0.8559\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.37584\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 1215s 110s/step - loss: 0.1906 - acc: 0.9529 - val_loss: 0.3939 - val_acc: 0.8689\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.37584\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 1218s 111s/step - loss: 0.1756 - acc: 0.9615 - val_loss: 0.4273 - val_acc: 0.8603\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.37584\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 1211s 110s/step - loss: 0.1858 - acc: 0.9523 - val_loss: 0.4150 - val_acc: 0.8673\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.37584\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 1201s 109s/step - loss: 0.1641 - acc: 0.9610 - val_loss: 0.4227 - val_acc: 0.8613\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.37584\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 1196s 109s/step - loss: 0.1383 - acc: 0.9711 - val_loss: 0.4285 - val_acc: 0.8638\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.37584\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 1203s 109s/step - loss: 0.1280 - acc: 0.9686 - val_loss: 0.4945 - val_acc: 0.8539\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.37584\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 1207s 110s/step - loss: 0.1356 - acc: 0.9708 - val_loss: 0.4289 - val_acc: 0.8673\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.37584\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 1227s 112s/step - loss: 0.1186 - acc: 0.9703 - val_loss: 0.4297 - val_acc: 0.8680\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.37584\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 1227s 112s/step - loss: 0.1223 - acc: 0.9724 - val_loss: 0.4505 - val_acc: 0.8620\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.37584\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 1213s 110s/step - loss: 0.1062 - acc: 0.9765 - val_loss: 0.4657 - val_acc: 0.8612\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.37584\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 1212s 110s/step - loss: 0.1038 - acc: 0.9797 - val_loss: 0.4827 - val_acc: 0.8567\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.37584\n",
            "Epoch 30/100\n",
            "10/11 [==========================>...] - ETA: 2s - loss: 0.0985 - acc: 0.9787"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvNEuXI9c9Wy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A140dojbEB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "  test_data_dir,\n",
        "  target_size=(img_width, img_height),\n",
        "  color_mode='rgb',\n",
        "  classes=classes,\n",
        "  class_mode='categorical',\n",
        "  batch_size=1,\n",
        "  shuffle=False)\n",
        "\n",
        "import pandas as pd\n",
        "vgg_model.load_weights('/content/gdrive/My Drive/ColabNotebooks/sutaba/models/weights.20-0.39-0.91-0.30-0.88.hdf5')\n",
        "loss = vgg_model.predict_generator(test_generator, steps=len(test_generator.classes), verbose=1)\n",
        "prob = pd.DataFrame(loss, columns=classes)\n",
        "prob['predict'] = prob.idxmax(axis=1)\n",
        "prob['actual'] = [classes[c] for c in test_generator.classes]\n",
        "prob['path'] = test_generator.filenames\n",
        "prob.to_csv('results.csv')\n",
        "\n",
        "print((prob['predict'] == prob['actual']).value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}