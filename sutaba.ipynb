{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sutaba.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpppk/colaboratory/blob/master/sutaba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W6D3jtL-PmK",
        "colab_type": "code",
        "outputId": "78e34ec4-b6b4-4dfb-e68a-046cdb6c8c86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# import tensorflow as tf\n",
        "# tf.test.gpu_device_name()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi6KAh2tExrV",
        "colab_type": "code",
        "outputId": "a3f1e8bc-174e-406c-8c45-1e2831d15011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !apt-get update && apt-get install imagemagick\n",
        "!cat /proc/uptime | awk '{print $1 /60 /60 /24 \"days (\" $1 \"sec)\"}'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.02971days (2566.94sec)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4f1ASW6_sqn",
        "colab_type": "code",
        "outputId": "b25b3794-2751-4854-f90b-2060004653e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Activation, Dropout, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# 分類するクラス\n",
        "classes = [\"sutaba\",\"ramen\", \"other\"]\n",
        "nb_classes = len(classes)\n",
        "#画像の大きさを設定\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# トレーニング用とバリデーション用の画像格納先（パスは自分で設定してください）\n",
        "base_path = \"/content/gdrive/My Drive/ColabNotebooks/sutaba\"\n",
        "train_data_dir = base_path + '/train'\n",
        "test_data_dir = base_path + '/test'\n",
        "aug_train_data_dir = base_path + '/aug_train'\n",
        "model_dir = base_path + '/models'\n",
        "# os.mkdir(aug_train_data_dir)\n",
        "\n",
        "#トレーニングデータ用の画像数\n",
        "def find_all_files(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            yield os.path.join(root, file)\n",
        "\n",
        "def count_file_num(p: str) -> int:\n",
        "  cnt = 0\n",
        "  for f in find_all_files(p):\n",
        "    cnt += 1\n",
        "  return cnt\n",
        "\n",
        "#バッチサイズ\n",
        "batch_size = 300\n",
        "#エポック数\n",
        "nb_epoch = 100\n",
        "validation_rate = 0.1\n",
        "\n",
        "sutaba_train_num = count_file_num(os.path.join(train_data_dir, classes[0]))\n",
        "ramen_train_num = count_file_num(os.path.join(train_data_dir, classes[1]))\n",
        "other_train_num = count_file_num(os.path.join(train_data_dir, classes[2]))\n",
        "class_weight = {0: other_train_num/sutaba_train_num, 1: other_train_num/ramen_train_num, 2: 1}\n",
        "nb_train_samples = sutaba_train_num + ramen_train_num + other_train_num\n",
        "\n",
        "sutaba_test_num = count_file_num(os.path.join(test_data_dir, classes[0]))\n",
        "ramen_test_num = count_file_num(os.path.join(test_data_dir, classes[1]))\n",
        "other_test_num = count_file_num(os.path.join(test_data_dir, classes[2]))\n",
        "nb_test_samples = sutaba_test_num + ramen_test_num + other_test_num\n",
        "\n",
        "print(sutaba_train_num, ramen_train_num, other_train_num, class_weight)\n",
        "print(sutaba_test_num, ramen_test_num, other_test_num)\n",
        "\n",
        "def get_latest_modified_file_path(dirname):\n",
        "  target = os.path.join(dirname, '*')\n",
        "  files = [(f, os.path.getmtime(f)) for f in glob(target)]\n",
        "  latest_modified_file_path = sorted(files, key=lambda files: files[1])[-1]\n",
        "  return latest_modified_file_path[0]\n",
        "\n",
        "def create_vgg16_from_weights(weights_path: str):\n",
        "  vgg16_model = create_vgg16()\n",
        "  vgg16_model.load_weights(get_latest_modified_file_path(weights_path))\n",
        "  return vgg16_model\n",
        "\n",
        "\n",
        "def create_vgg16():\n",
        "  # VGG16のロード。FC層は不要なので include_top=False\n",
        "  input_tensor = Input(shape=(img_width, img_height, 3))\n",
        "  vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
        "\n",
        "  # VGG16の図の緑色の部分（FC層）の作成\n",
        "  top_model = Sequential()\n",
        "  top_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\n",
        "  top_model.add(Dense(256, activation='relu'))\n",
        "  top_model.add(Dropout(0.5))\n",
        "  top_model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "  # VGG16とFC層を結合してモデルを作成（完成図が上の図）\n",
        "  # vgg_model = Model()\n",
        "  vgg_model = Model(input=vgg16.input, output=top_model(vgg16.output))\n",
        "\n",
        "  # VGG16の図の青色の部分は重みを固定（frozen）\n",
        "  for layer in vgg_model.layers[:15]:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # 多クラス分類を指定\n",
        "  vgg_model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=optimizers.SGD(lr=1e-3, momentum=0.9),\n",
        "            metrics=['accuracy'])\n",
        "  return vgg_model\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "  rescale=1.0 / 255,\n",
        "  #すでに画像の水増し済みの方は、下記２行は必要ありません。\n",
        "  zoom_range=0.2,\n",
        "  horizontal_flip=True,\n",
        "  validation_split=validation_rate\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "  train_data_dir,\n",
        "  target_size=(img_width, img_height),\n",
        "  color_mode='rgb',\n",
        "  classes=classes,\n",
        "  class_mode='categorical',\n",
        "  batch_size=batch_size,\n",
        "  # save_to_dir=aug_train_data_dir,\n",
        "  shuffle=True,\n",
        "  subset='training'\n",
        ")\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "  train_data_dir,\n",
        "  target_size=(img_width, img_height),\n",
        "  color_mode='rgb',\n",
        "  classes=classes,\n",
        "  class_mode='categorical',\n",
        "  batch_size=batch_size,\n",
        "  # save_to_dir=aug_train_data_dir,\n",
        "  shuffle=True,\n",
        "  subset='validation'\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "900 550 2387 {0: 2.652222222222222, 1: 4.34, 2: 1}\n",
            "73 67 70\n",
            "Found 3454 images belonging to 3 classes.\n",
            "Found 383 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "490F074Jtv0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mogrify \\\n",
        "  -path /content/gdrive/'My Drive'/ColabNotebooks/sutaba/train/sutaba \\\n",
        "  -define jpeg:size=224x224 \\\n",
        "  -thumbnail 224x224^ \\\n",
        "  -gravity center \\\n",
        "  -extent 224x224 \\\n",
        "  /content/gdrive/'My Drive'/ColabNotebooks/sutaba/train/sutaba-original/*.jpg\n",
        "# convert -define jpeg:size=200x200 original.jpeg  -thumbnail 100x100^ -gravity center -extent 100x100  thumbnail.jpeg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn_8XIv2ARY8",
        "colab_type": "code",
        "outputId": "307be7ca-223d-4e3e-8f19-0827c419c922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        }
      },
      "source": [
        "\n",
        "import keras\n",
        "vgg_model = create_vgg16_from_weights(model_dir)\n",
        "n = datetime.datetime.now()\n",
        "nstr = f'{n.year}-{n.month:02}-{n.day:02}_{n.hour:02}-{n.minute:02}-{n.second:02}'\n",
        "fpath = base_path + f'/models/{nstr}' + 'weights.{epoch:02d}-{loss:.2f}-{acc:.2f}-{val_loss:.2f}-{val_acc:.2f}.hdf5'\n",
        "cp_cb = keras.callbacks.ModelCheckpoint(filepath=fpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "# Fine-tuning\n",
        "history = vgg_model.fit_generator(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    # samples_per_epoch=nb_train_samples,\n",
        "    samples_per_epoch=len(train_generator.classes),\n",
        "    nb_epoch=nb_epoch,\n",
        "    callbacks=[cp_cb],\n",
        "    nb_val_samples=len(validation_generator.classes),\n",
        "    class_weight=class_weight\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0810 03:26:11.347245 140710279047040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0810 03:26:11.366352 140710279047040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0810 03:26:11.384002 140710279047040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0810 03:26:11.431067 140710279047040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0810 03:26:12.485852 140710279047040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0810 03:26:12.487463 140710279047040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0810 03:26:14.754296 140710279047040 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n",
            "W0810 03:26:14.848085 140710279047040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-587f3a8296e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvgg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vgg16_from_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{n.year}-{n.month:02}-{n.day:02}_{n.hour:02}-{n.minute:02}-{n.second:02}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'/models/{nstr}'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'weights.{epoch:02d}-{loss:.2f}-{acc:.2f}-{val_loss:.2f}-{val_acc:.2f}.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvNEuXI9c9Wy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A140dojbEB1",
        "colab_type": "code",
        "outputId": "1b36a27c-8549-4ca3-c16b-2d80b19b8fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "  test_data_dir,\n",
        "  target_size=(img_width, img_height),\n",
        "  color_mode='rgb',\n",
        "  classes=classes,\n",
        "  class_mode='categorical',\n",
        "  batch_size=1,\n",
        "  shuffle=False)\n",
        "\n",
        "import pandas as pd\n",
        "vgg_model = create_vgg16_from_weights(model_dir)\n",
        "loss = vgg_model.predict_generator(test_generator, steps=len(test_generator.classes), verbose=1)\n",
        "prob = pd.DataFrame(loss, columns=classes)\n",
        "prob['predict'] = prob.idxmax(axis=1)\n",
        "prob['actual'] = [classes[c] for c in test_generator.classes]\n",
        "prob['path'] = test_generator.filenames\n",
        "prob.to_csv('results.csv')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0820 06:20:25.875839 139936511432576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0820 06:20:25.912021 139936511432576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0820 06:20:25.920267 139936511432576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0820 06:20:25.958105 139936511432576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 210 images belonging to 3 classes.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0820 06:20:28.380908 139936511432576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0820 06:20:28.381812 139936511432576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0820 06:20:31.646105 139936511432576 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n",
            "W0820 06:20:31.713658 139936511432576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "210/210 [==============================] - 69s 329ms/step\n",
            "True     186\n",
            "False     24\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "purjqCjhss1S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "074df709-eb78-4c68-ce41-0b037866bf21"
      },
      "source": [
        "result = (prob['predict'] == prob['actual']).value_counts()\n",
        "print(\"all:\", result[True] / len(prob) * 100, \"%\")\n",
        "\n",
        "sutaba_prob = prob[prob['actual'] == 'sutaba']\n",
        "sutaba_result = (sutaba_prob['predict'] == sutaba_prob['actual']).value_counts()\n",
        "print(\"sutaba:\", sutaba_result[True] / len(sutaba_prob) * 100, \"%\")\n",
        "\n",
        "ramen_prob = prob[prob['actual'] == 'ramen']\n",
        "ramen_result = (ramen_prob['predict'] == ramen_prob['actual']).value_counts()\n",
        "print(\"ramen:\", ramen_result[True] / len(ramen_prob) * 100, \"%\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all: 88.57142857142857 %\n",
            "sutaba: 90.41095890410958 %\n",
            "ramen: 83.5820895522388 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}